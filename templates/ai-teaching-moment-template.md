# AI Teaching Moment Template

This template helps teams document and share AI learning experiences, following the three key patterns of successful AI implementation:
1. Standards Before Speed: Document clear, reusable practices
2. Experience Over Output: Focus on learning and understanding
3. Fluency Over Dependency: Share knowledge and build collective expertise

## Learning Documentation Framework

**Template Structure:**
```markdown
# AI Learning Entry: [Brief Description]

## Context
**Date:** [YYYY-MM-DD]
**Team Member:** [Name]
**AI Tool Used:** [Tool name and model]
**Task Type:** [Feature/Bug Fix/Refactor/Learning]
**Complexity Level:** [Simple/Medium/Complex]
**Implementation Week:** [Week X of AI adoption - important for 11-week productivity benchmark]
**Standards Alignment:** [Which team standards/guidelines were followed]
**Risk Level:** [Low/Medium/High - for security consideration]

## What We Tried
**Objective:** [What we wanted to achieve]

**First Draft Assessment:**
- **70/30 Split:** [Was this in the "easy 70%" or "complex 30%" of implementation?]
- **Architecture Impact:** [How did this affect system architecture/design?]
- **Security Implications:** [Any security considerations/scans needed]

**Approach 1:**
- **Prompt Strategy:** [Exact prompt or approach description]
- **AI Tool Configuration:** [Model, temperature, settings]
- **Result:** [What happened - success/partial/failure]
- **Quality Score:** [1-5 based on CARE framework]
- **Context Management:** [How we handled the context window limitations]
- **Standards Compliance:** [How well it met documented team standards]
- **Security Scan Results:** [Results of security scanning if applicable]
- **Trust Level:** [1-5 scale on team's confidence in the output]
  - Completeness: [1-5]
  - Accuracy: [1-5]  
  - Relevance: [1-5]
  - Efficiency: [1-5]

**Approach 2:** [If applicable, repeat structure above]

## What Worked Well
- **Standards Alignment:** [How well we followed team guidelines]
- **Developer Experience:** [Learning and confidence outcomes]
- **Knowledge Sharing:** [How this contributed to team fluency]
- **Security & Quality:** [How we maintained security and quality]

## What Didn't Work
- **Standards Gaps:** [Where our guidelines weren't clear enough]
- **Context Window Issues:** [Problems with AI losing broader context]
- **Security Concerns:** [Any vulnerabilities or risks identified]
- **Trust Barriers:** [Areas where team confidence was low]

## Root Cause Analysis
**Why did it work/not work?**
- [Technical reasons - model limitations, prompt design, context issues]
- [Process reasons - workflow integration, timing, team coordination]
- [Knowledge gaps - areas where human expertise was needed]

## Lessons Learned
**For Future Similar Tasks:**
- [Specific recommendations for next time]
- [Updated prompt templates or strategies]
- [Context setup improvements]
- [Process refinements]

**For Team Knowledge:**
- [Insights relevant to other team members]
- [Patterns that could apply to other use cases]
- [Anti-patterns to avoid]

## Reusable Assets Created
- [ ] **Prompt Template:** [Link or embed reusable prompt]
- [ ] **Context Setup:** [Reusable context configuration]
- [ ] **Quality Checklist:** [Specific validation steps for this type of task]
- [ ] **Process Improvement:** [Workflow enhancement for team adoption]

## Knowledge Sharing Actions
- [ ] Shared in team chat/Slack with summary
- [ ] Added to team wiki/knowledge base
- [ ] Presented in team standup/meeting
- [ ] Updated team AI guidelines document
- [ ] Created/updated prompt library
- [ ] Demo'd in AI learning session
- [ ] Added to team's AI pattern library
- [ ] Security findings shared with security team
- [ ] Updated standards documentation if needed

## Impact Measurement
**Standards Metrics:**
- Guidelines followed: [Yes/No with details]
- Security scan results: [Pass/Fail/Issues]
- Code revert rate: [% of AI-generated code reverted]

**Experience Metrics:**
- Developer confidence: [1-5 scale]
- Learning outcomes: [New skills/insights gained]
- Trust level: [1-5 scale]

**Fluency Metrics:**
- Knowledge shared: [How/where/with whom]
- Team adoption: [Who else is using this approach]
- Reuse potential: [High/Medium/Low]

## Follow-up Actions
- [ ] [Specific action item 1 with owner and deadline]
- [ ] [Specific action item 2 with owner and deadline]
- [ ] Schedule follow-up review in [timeframe]
```

## Simplified Quick Learning Template

**For rapid knowledge capture (5-minute version):**

```markdown
## Quick AI Learning: [One-line description]
**Date:** [MM/DD] | **Week:** [X/11] | **Risk Level:** [Low/Med/High]

**üìè Standards Check:**
- Guidelines followed: [Yes/No]
- Security scan: [Pass/Fail]
- Trust level: [1-5]

**üí° What Worked:**
- [Standards alignment]
- [Developer experience]
- [Knowledge sharing]

**‚ùå What Didn't:**
- [Main challenge]
- [Security concerns]
- [Trust issues]

**üì§ Shared:** [Where/How]
```